{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "293px",
        "left": "996px",
        "right": "20px",
        "top": "120px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Funções finais.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ITZ9gfqDeri",
        "colab_type": "text"
      },
      "source": [
        "***Ashrae utility consumption - Kaggle Competition***\n",
        "\n",
        "This notebook was created in order to participate in the Kaggle competition for the Ashrae utility consumption prediction. The challenge was based on predicting the next year levels of consumption for 4 utilities: hot water, cold water, steam and electricity; based on a 1 year historical data. The data was split in 5 datasets, 2 for weather (train and test), 1 for buildings identification, 1 for train data and 1 for test data. \n",
        "\n",
        "This project consisted on several steps on data exploration, cleaning and wrangling as well as several approaches on the modelling phase. Although it wasn't awarded any prizes, it was a very interesting experience and it served many ideas for application in real cases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:13:56.451552Z",
          "start_time": "2019-12-05T20:13:54.781296Z"
        },
        "id": "SxTXPATMkk89",
        "colab_type": "code",
        "outputId": "abd411eb-b260-4585-a3e7-a421135dd10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "'''\n",
        "Importing of libraries used and definition of standards\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as ms\n",
        "import io\n",
        "\n",
        "\n",
        "import random\n",
        "from datetime import datetime as dt\n",
        "from time import time\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "import gc\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "\n",
        "import tqdm\n",
        "\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_rows = 100\n",
        "\n",
        "\n",
        "#!apt-get -qq install --no-install-recommends nvidia-375\n",
        "#!apt-get -qq install --no-install-recommends nvidia-opencl-icd-375 nvidia-opencl-dev opencl-headers\n",
        "#!apt-get -qq install --no-install-recommends git cmake build-essential libboost-dev libboost-system-dev libboost-filesystem-dev\n",
        "#!pip3 install -qq lightgbm --install-option=--gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rJqX8W2rudS",
        "colab_type": "code",
        "outputId": "a66fc76f-0c50-49e0-9ace-545682f39675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "Link to the Google drive repository for importing the datasets\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ley999LPTjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Reading the csv files for the training data and transforming them into pandas dataframes\n",
        "'''\n",
        "weather_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Kaggle/data/weather_train.csv')\n",
        "\n",
        "build = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Kaggle/data/building_metadata.csv').fillna(-1)\n",
        "le = LabelEncoder()\n",
        "build.primary_use = le.fit_transform(build.primary_use)\n",
        "\n",
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Kaggle/data/train.csv')\n",
        "train_dates = train.timestamp.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvaxIg9SPgb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Reading the csv files for the testing data and transforming them into pandas dataframes\n",
        "'''\n",
        "build = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Kaggle/data/building_metadata.csv').fillna(-1)\n",
        "le = LabelEncoder()\n",
        "build.primary_use = le.fit_transform(build.primary_use)\n",
        "\n",
        "weather_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Kaggle/data/weather_test.csv')\n",
        "\n",
        "test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Kaggle/data/test.csv')\n",
        "test_dates = test.timestamp.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:13:56.585212Z",
          "start_time": "2019-12-05T20:13:56.560986Z"
        },
        "id": "0Rdqc5Jjkk9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Definition of the function to reduce the memory usage of the datasets so it can be manipulated without major problems.\n",
        "This function was was based on this kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
        "'''\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:14:46.170833Z",
          "start_time": "2019-12-05T20:14:46.162932Z"
        },
        "id": "x3_Exsi7kk9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function is responsible for normalizing the registered values from one of the buildings for which the readings were found completely out of the standards,\n",
        "during the exploratory data analysis. The way of normalizing the data was based on scaling the readings so they were transformed to the same order as the\n",
        "remaining dataset.\n",
        "'''\n",
        "def regularize_build(df):\n",
        "    df.loc[df[(df.building_id==1099) & (df.meter==2)].index, 'meter_reading'] = df[\n",
        "        (df.building_id==1099) & (df.meter==2)]['meter_reading'].values/1e4\n",
        "    df.loc[df[df.building_id==1099][df.meter==2][\n",
        "        df[df.building_id==1099][df.meter==2].meter_reading < 10].index, 'meter_reading'] = 0\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:14:46.875095Z",
          "start_time": "2019-12-05T20:14:46.852718Z"
        },
        "id": "wNga_93-kk9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function is responsible for the feature engineering process on the weather dataset. It is comprised of several steps, such as:\n",
        "- regularizing negative values for some attributes that can't be negative(logically)\n",
        "- creating a set of features that describes if the values were filled originally\n",
        "- regularizing the time intervals recorded on the dataset\n",
        "- filling the missing values with interpolations\n",
        "'''\n",
        "def weather_transform(weather, dates):\n",
        "    weather['wind_direction'] = np.sin(weather.wind_direction*np.pi/180)\n",
        "    weather.precip_depth_1_hr = weather.precip_depth_1_hr.apply(lambda x: x if x>= 0 else None)\n",
        "    \n",
        "    weather['had_air'] = (pd.isna(weather.air_temperature)==False)*1\n",
        "    weather['had_cloud'] = (pd.isna(weather.cloud_coverage)==False)*1\n",
        "    weather['had_dew'] = (pd.isna(weather.dew_temperature)==False)*1\n",
        "    weather['had_precip'] = (pd.isna(weather.precip_depth_1_hr)==False)*1\n",
        "    weather['had_pressure'] = (pd.isna(weather.sea_level_pressure)==False)*1\n",
        "    weather['had_wind'] = (pd.isna(weather.wind_direction)==False)*1\n",
        "    weather['had_speed'] = (pd.isna(weather.wind_speed)==False)*1\n",
        "\n",
        "    weather = weather.set_index(pd.DatetimeIndex(weather['timestamp'])).groupby(\n",
        "        'site_id').apply(lambda group: group.reindex(pd.DatetimeIndex(dates),fill_value=np.nan)).drop(\n",
        "        'site_id', axis=1).reset_index()\n",
        "    weather = weather.drop('timestamp', axis=1).rename(columns={\"level_1\": \"timestamp\"})\n",
        "    weather = weather.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
        "\n",
        "    weather['cloud_coverage'] = weather[['timestamp', 'cloud_coverage']].groupby(['timestamp']).apply(\n",
        "        lambda group : group.fillna(group.mean()))['cloud_coverage'].values.round()\n",
        "    weather['precip_depth_1_hr'] = weather[['timestamp', 'precip_depth_1_hr']].groupby(['timestamp']).apply(\n",
        "        lambda group : group.fillna(group.mean()))['precip_depth_1_hr'].values\n",
        "    weather['sea_level_pressure'] = weather[['timestamp', 'sea_level_pressure']].groupby(['timestamp']).apply(\n",
        "        lambda group : group.fillna(group.mean()))['sea_level_pressure'].values\n",
        "    \n",
        "    return weather"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:14:47.593997Z",
          "start_time": "2019-12-05T20:14:47.584050Z"
        },
        "id": "vm9t4dXTkk9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function merges all three datasets, sorting the values at the end\n",
        "'''\n",
        "def df_merge(data, weather, build, dates):\n",
        "    data['timestamp'] = pd.to_datetime(data.timestamp)\n",
        "    \n",
        "    data = data.set_index(pd.DatetimeIndex(data['timestamp'])).groupby(\n",
        "            ['meter', 'building_id']).apply(lambda group: group.reindex(pd.DatetimeIndex(dates),fill_value=0)).drop(\n",
        "            ['meter', 'building_id'], axis=1).reset_index().drop('timestamp',axis=1).rename(columns={\"level_2\": \"timestamp\"})\n",
        "    \n",
        "    data = pd.merge(pd.merge(data, build, left_on='building_id', right_on='building_id', how='left')\n",
        "             , weather, left_on=['site_id', 'timestamp'], right_on=['site_id', 'timestamp'], how='left')\n",
        "    \n",
        "    data = data.sort_values(['meter', 'building_id', 'timestamp'])\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:14:48.281156Z",
          "start_time": "2019-12-05T20:14:48.266393Z"
        },
        "id": "YDTelEAgkk9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function creates meaningful date/time features in order to capture sazonality or frequency of the behavior\n",
        "'''\n",
        "def time_columns(df):\n",
        "    df['day_of_year'] = (df['timestamp'].dt.dayofyear-1)\n",
        "    df['week_of_year'] = df['timestamp'].dt.week-1\n",
        "    df['month'] = df['timestamp'].dt.month-1\n",
        "    df['weekend'] = pd.cut(df['timestamp'].dt.weekday, bins=[-0.1, 4, 6], labels =[0, 1])\n",
        "    df['day_period'] = pd.cut(df['timestamp'].dt.hour, bins=[-0.1, 6, 12, 18, 24], labels=[0, 1, 2, 3])\n",
        "    df['season'] = pd.cut(df['timestamp'].dt.dayofyear + 11 - 366*(df['timestamp'].dt.dayofyear > 355),\n",
        "                         bins=[0, 91, 183, 275, 366], labels=[0, 1, 2, 3])\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:14:49.002689Z",
          "start_time": "2019-12-05T20:14:48.987709Z"
        },
        "id": "yRqPNeVKkk9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function fills the missing values from the sensors readings using a set of of progressive transformations, trying to capture characteristics that\n",
        "may be particular to seasons or locations.\n",
        "'''\n",
        "def df_transform(df):\n",
        "    df['meter_reading'] = df['meter_reading'].replace(0, np.nan)\n",
        "    \n",
        "    df['meter_reading'] = df[['meter', 'building_id', 'season', 'meter_reading']].groupby(\n",
        "        ['meter', 'building_id', 'season']).apply(lambda group: group.fillna(group.mean()))['meter_reading'].values\n",
        "    df['meter_reading'] = df[['meter', 'building_id', 'meter_reading']].groupby(\n",
        "        ['meter', 'building_id']).apply(lambda group: group.fillna(group.mean()))['meter_reading'].values\n",
        "    \n",
        "    df['meter_reading'] = df[['meter', 'building_id', 'timestamp', 'meter_reading']].groupby(\n",
        "        ['meter', 'building_id']).apply(lambda group: group.set_index('timestamp').rolling('3H').median())['meter_reading'].values\n",
        "    \n",
        "    df['log_meter_reading'] = np.log1p(df[\"meter_reading\"])\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzZJnvk7cOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function creates a deep learning model using a Keras sequential one for predicting the results. The neural network uses a set of embedding layers for \n",
        "categorical features.\n",
        "'''\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.losses import mean_squared_error as mse_loss\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "categoricals = [\"site_id\", \"building_id\", \"primary_use\", \"cloud_coverage\", \"hour\", \"weekday\", 'day_of_year', 'week_of_year', 'month', 'day_period', \n",
        "                'season',  \"meter\"]\n",
        "\n",
        "drop_cols = [\"sea_level_pressure\", \"wind_direction\"]\n",
        "\n",
        "numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\", 'wind_speed']\n",
        "\n",
        "all_features = categoricals + numericals\n",
        "\n",
        "def model(dense_dim_1=128, dense_dim_2=64, dense_dim_3=32, dense_dim_4=32, dense_dim_5=16\n",
        "          dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.01):\n",
        "\n",
        "    #Inputs\n",
        "    site_id = Input(shape=[1], name=\"site_id\")\n",
        "    building_id = Input(shape=[1], name=\"building_id\")\n",
        "    primary_use = Input(shape=[1], name=\"primary_use\")\n",
        "    cloud_coverage = Input(shape=[1], name=\"cloud_coverage\")\n",
        "    hour = Input(shape=[1], name=\"hour\")\n",
        "    weekday = Input(shape=[1], name=\"weekday\")\n",
        "    day_of_year = Input(shape=[1], name=\"day_of_year\")\n",
        "    week_of_year = Input(shape=[1], name=\"week_of_year\")\n",
        "    month = Input(shape=[1], name=\"month\")\n",
        "    day_period = Input(shape=[1], name=\"day_period\")\n",
        "    season = Input(shape=[1], name=\"season\")\n",
        "    meter = Input(shape=[1], name=\"meter\")\n",
        "    \n",
        "    square_feet = Input(shape=[1], name=\"square_feet\")\n",
        "    year_built = Input(shape=[1], name=\"year_built\")\n",
        "    air_temperature = Input(shape=[1], name=\"air_temperature\")\n",
        "    dew_temperature = Input(shape=[1], name=\"dew_temperature\")\n",
        "    precip = Input(shape=[1], name=\"precip_depth_1_hr\")\n",
        "    floor_count = Input(shape=[1], name=\"floor_count\")\n",
        "    wind_speed = Input(shape=[1], name=\"wind_speed\")\n",
        "   \n",
        "    #Embeddings layers\n",
        "    emb_site_id = Embedding(16, 8)(site_id)\n",
        "    emb_building_id = Embedding(1449, 50)(building_id)\n",
        "    emb_primary_use = Embedding(16, 8)(primary_use)\n",
        "    emb_cloud_coverage = Embedding(16, 5)(cloud_coverage)\n",
        "    emb_hour = Embedding(24, 12)(hour)\n",
        "    emb_weekday = Embedding(7, 3)(weekday)\n",
        "    emb_day_of_year = Embedding(366, 50)(day_of_year)\n",
        "    emb_week_of_year = Embedding(52, 26)(week_of_year)\n",
        "    emb_month = Embedding(12, 6)(month)\n",
        "    emb_day_period = Embedding(4, 2)(day_period)\n",
        "    emb_season = Embedding(4, 2)(season)\n",
        "    emb_meter = Embedding(4, 2)(meter)\n",
        "\n",
        "    concat_emb = concatenate([\n",
        "           Flatten() (emb_site_id)\n",
        "         , Flatten() (emb_building_id)\n",
        "         , Flatten() (emb_primary_use)\n",
        "         , Flatten() (emb_cloud_coverage)\n",
        "         , Flatten() (emb_hour)\n",
        "         , Flatten() (emb_weekday)\n",
        "         , Flatten() (emb_day_of_year)\n",
        "         , Flatten() (emb_week_of_year)\n",
        "         , Flatten() (emb_month)\n",
        "         , Flatten() (emb_day_period)\n",
        "         , Flatten() (emb_season)\n",
        "         , Flatten() (emb_meter)\n",
        "    ])\n",
        "    \n",
        "    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n",
        "    categ = BatchNormalization()(categ)\n",
        "    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n",
        "    categ = BatchNormalization()(categ)\n",
        "    categ = Dropout(dropout2)(Dense(dense_dim_3,activation='relu') (categ))\n",
        "\n",
        "    #main layer\n",
        "    main_l = concatenate([\n",
        "          categ\n",
        "        , square_feet\n",
        "        , year_built\n",
        "        , air_temperature\n",
        "        , dew_temperature\n",
        "        , precip\n",
        "        , floor_count\n",
        "        , wind_speed\n",
        "    ])\n",
        "    \n",
        "    main_l = Dropout(dropout3)(Dense(dense_dim_4,activation='relu') (main_l))\n",
        "    main_l = BatchNormalization()(main_l)\n",
        "    main_l = Dropout(dropout4)(Dense(dense_dim_5,activation='relu') (main_l))\n",
        "    \n",
        "    #output\n",
        "    output = Dense(1) (main_l)\n",
        "\n",
        "    model = Model([site_id,\n",
        "                   building_id, \n",
        "                   primary_use,\n",
        "                   cloud_coverage,\n",
        "                   hour,\n",
        "                   weekday,\n",
        "                   day_of_year,\n",
        "                   week_of_year,\n",
        "                   month,\n",
        "                   day_period,\n",
        "                   season,\n",
        "                   meter,\n",
        "                   square_feet,\n",
        "                   year_built,\n",
        "                   air_temperature,\n",
        "                   dew_temperature,\n",
        "                   precip,\n",
        "                   floor_count,\n",
        "                   wind_speed], output)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr=lr),\n",
        "                  loss= mse_loss,\n",
        "                  metrics=[root_mean_squared_error])\n",
        "    return model\n",
        "\n",
        "'''\n",
        "This function represents the error function used to approximate the solutions\n",
        "'''\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n",
        "\n",
        "def get_keras_data(df, num_cols, cat_cols):\n",
        "    cols = num_cols + cat_cols\n",
        "    X = {col: np.array(df[col]) for col in cols}\n",
        "    return X\n",
        "\n",
        "'''\n",
        "This function trains the model\n",
        "'''\n",
        "def train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, patience=3):\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint(\"model_\" + str(fold) + \".hdf5\",\n",
        "                                       save_best_only=True, verbose=1, monitor='val_root_mean_squared_error', mode='min')\n",
        "\n",
        "    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n",
        "                            validation_data=(X_v, y_valid), verbose=1,\n",
        "                            callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    keras_model = load_model(\"model_\" + str(fold) + \".hdf5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
        "    \n",
        "    return keras_model\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:14:49.739736Z",
          "start_time": "2019-12-05T20:14:49.730395Z"
        },
        "id": "W4hV217Mkk9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function transforms some of the date/time features in order to normalize their values as well as capturing the periodic behavior\n",
        "'''\n",
        "def sen_cos(df, colunas = ['day_of_year', 'week_of_year', 'month', 'day_period', 'season']):\n",
        "    '''\n",
        "    df: dataframe com colunas para transformar em sen cos\n",
        "    colunas: colunas a serem transformadas\n",
        "    return: dataframe com as colunas sen cos\n",
        "    '''\n",
        "    df[colunas] = df[colunas].astype('int64')/df[colunas].astype('int64').max()\n",
        "    df[[col +'_sen' for col in colunas]] = df[colunas].apply(lambda x: np.sin(2*np.pi*x))\n",
        "    df[[col +'_cos' for col in colunas]] = df[colunas].apply(lambda x: np.cos(2*np.pi*x))\n",
        "    \n",
        "    \n",
        "    return df.drop(colunas, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-05T20:22:13.964728Z",
          "start_time": "2019-12-05T20:17:10.402138Z"
        },
        "scrolled": true,
        "id": "JpDACrLykk9d",
        "colab_type": "code",
        "outputId": "74f9e361-59a8-4094-f484-1bea507f877b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "'''\n",
        "Application of the transformations over the training dataset\n",
        "'''\n",
        "weather_train = weather_transform(weather_train, train_dates)\n",
        "print('weather_train ready')\n",
        "train = regularize_build(train)\n",
        "df_train = df_merge(train, weather_train, build, train_dates)\n",
        "print('datasets merged')\n",
        "df_train = time_columns(df_train)\n",
        "print('time features created')\n",
        "df_train = df_transform(df_train)\n",
        "df_train = sen_cos(df_train, colunas = ['day_of_year', 'week_of_year', 'month', 'day_period', 'season'])\n",
        "df_train = reduce_mem_usage(df_train, use_float16=False)\n",
        "print('df_train ready')\n",
        "\n",
        "del le, weather_train, train\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weather_train ready\n",
            "datasets merged\n",
            "time features created\n",
            "Memory usage of dataframe is 5602.42 MB\n",
            "Memory usage after optimization is: 2751.37 MB\n",
            "Decreased by 50.9%\n",
            "df_train ready\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHqEaKyNkk9h",
        "colab_type": "code",
        "outputId": "bf74a156-1573-4bd3-9ec7-7a262cbe6931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "'''\n",
        "Application of the transformations over the training dataset\n",
        "'''\n",
        "weather_test = weather_transform(weather_test, test_dates)\n",
        "print('weather_test ready')\n",
        "df_test = df_merge(test, weather_test, build, test_dates)\n",
        "print('datasets merged')\n",
        "df_test = time_columns(df_test)\n",
        "print('time features created')\n",
        "del weather_test, build, test\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weather_test ready\n",
            "datasets merged\n",
            "time features created\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4BFvS7AdMIS",
        "colab_type": "code",
        "outputId": "17a316d8-f7e1-4879-cdfc-11c3636ba237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df_test = reduce_mem_usage(df_test, use_float16=False)\n",
        "df_test = sen_cos(df_test, colunas = ['day_of_year', 'week_of_year', 'month', 'day_period', 'season'])\n",
        "df_test = reduce_mem_usage(df_test, use_float16=False)\n",
        "print('df_test ready')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 8708.74 MB\n",
            "Memory usage after optimization is: 3976.59 MB\n",
            "Decreased by 54.3%\n",
            "Memory usage of dataframe is 6919.27 MB\n",
            "Memory usage after optimization is: 5328.63 MB\n",
            "Decreased by 23.0%\n",
            "df_test ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G2KoeUakk-K",
        "colab_type": "text"
      },
      "source": [
        "**Xgboost on the entire dataset**\n",
        "\n",
        "In this section it is attempted to predict the meters values over the entire dataset using the LightXGboost model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pclsoPNbkk-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features = [\n",
        "    \"building_id\",\n",
        "    \"site_id\",\n",
        "    \"primary_use\",\n",
        "    \"meter\"\n",
        "    #'cloud_coverage'\n",
        "    #\"cluster\"\n",
        "]\n",
        "\n",
        "all_features = [col for col in df_train.columns if col not in [\"timestamp\", \n",
        "                                                               #\"site_id\", \n",
        "                                                               \"meter_reading\",\n",
        "                                                               \"log_meter_reading\",\n",
        "                                                               #\"building_id\"\n",
        "                                                              ]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFDfu5cMkk-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'boosting_type': 'gbdt',\n",
        "          'max_depth' : 4,\n",
        "          'nthread': -1,\n",
        "          'num_leaves': 50,\n",
        "          'learning_rate': lambda iter: 0.05 * (0.99 ** iter),\n",
        "          'random_state' : 501,\n",
        "          #'device_type' : 'gpu',\n",
        "          'metric' : 'rmse',\n",
        "          #'subsample_for_bin': 200,\n",
        "          #'subsample': 1,\n",
        "          #'subsample_freq': 1,\n",
        "          'colsample_bytree': 0.85\n",
        "          #'reg_alpha': 2,\n",
        "          #'reg_lambda': 10,\n",
        "          #'min_split_gain': 0.5,\n",
        "          #'min_child_weight': 1,\n",
        "          #'min_child_samples': 5,\n",
        "          #'scale_pos_weight': 1\n",
        "          #'num_class' : 1\n",
        "          }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oycu2VQ9PIdl",
        "colab_type": "code",
        "outputId": "12b644d3-e323-424b-e18e-28e528678833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "X_train = df_train.reset_index(drop=True)\n",
        "y_train = X_train[\"log_meter_reading\"]\n",
        "y_pred_train = np.zeros(X_train.shape[0])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "X_train, X_valid = X_train[all_features], X_test[all_features]\n",
        "y_train, y_valid = y_train, y_test\n",
        "del X_test, y_test\n",
        "dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "dvalid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "print('training')\n",
        "model = lgb.train(params, train_set=dtrain, \n",
        "                  num_boost_round=1000, \n",
        "                  valid_sets=[dtrain, dvalid], \n",
        "                  verbose_eval=25,\n",
        "                  early_stopping_rounds=50)\n",
        "print('finished')\n",
        "del dtrain, dvalid\n",
        "\n",
        "model.save_model('/content/drive/My Drive/Colab Notebooks/Kaggle/data/model_train_test.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[25]\tvalid_0's rmse: 1.59995\n",
            "[50]\tvalid_0's rmse: 1.46331\n",
            "[75]\tvalid_0's rmse: 1.3634\n",
            "[100]\tvalid_0's rmse: 1.27725\n",
            "[125]\tvalid_0's rmse: 1.20958\n",
            "[150]\tvalid_0's rmse: 1.15412\n",
            "[175]\tvalid_0's rmse: 1.10944\n",
            "[200]\tvalid_0's rmse: 1.07295\n",
            "[225]\tvalid_0's rmse: 1.03631\n",
            "[250]\tvalid_0's rmse: 0.997021\n",
            "[275]\tvalid_0's rmse: 0.966245\n",
            "[300]\tvalid_0's rmse: 0.936952\n",
            "[325]\tvalid_0's rmse: 0.918862\n",
            "[350]\tvalid_0's rmse: 0.901057\n",
            "[375]\tvalid_0's rmse: 0.889061\n",
            "[400]\tvalid_0's rmse: 0.879918\n",
            "[425]\tvalid_0's rmse: 0.87179\n",
            "[450]\tvalid_0's rmse: 0.864283\n",
            "[475]\tvalid_0's rmse: 0.858429\n",
            "[500]\tvalid_0's rmse: 0.852156\n",
            "[525]\tvalid_0's rmse: 0.847663\n",
            "[550]\tvalid_0's rmse: 0.83679\n",
            "[575]\tvalid_0's rmse: 0.832279\n",
            "[600]\tvalid_0's rmse: 0.823469\n",
            "[625]\tvalid_0's rmse: 0.817131\n",
            "[650]\tvalid_0's rmse: 0.811124\n",
            "[675]\tvalid_0's rmse: 0.804699\n",
            "[700]\tvalid_0's rmse: 0.799043\n",
            "[725]\tvalid_0's rmse: 0.795052\n",
            "[750]\tvalid_0's rmse: 0.790274\n",
            "[775]\tvalid_0's rmse: 0.786343\n",
            "[800]\tvalid_0's rmse: 0.78172\n",
            "[825]\tvalid_0's rmse: 0.777605\n",
            "[850]\tvalid_0's rmse: 0.773555\n",
            "[875]\tvalid_0's rmse: 0.769661\n",
            "[900]\tvalid_0's rmse: 0.766543\n",
            "[925]\tvalid_0's rmse: 0.763512\n",
            "[950]\tvalid_0's rmse: 0.75891\n",
            "[975]\tvalid_0's rmse: 0.755418\n",
            "[1000]\tvalid_0's rmse: 0.752225\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\tvalid_0's rmse: 0.752225\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnFJhg3UW9OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = lgb.Booster(model_file='/content/drive/My Drive/Colab Notebooks/Kaggle/data/model_train_test.txt')\n",
        "categorical_features = [\n",
        "    \"building_id\",\n",
        "    \"site_id\",\n",
        "    \"primary_use\",\n",
        "    \"meter\"\n",
        "    #'cloud_coverage'\n",
        "    #\"cluster\"\n",
        "]\n",
        "\n",
        "all_features = [col for col in df_test.columns if col not in [\"timestamp\", \n",
        "                                                               #\"site_id\", \n",
        "                                                               \"meter_reading\",\n",
        "                                                               \"log_meter_reading\",\n",
        "                                                               #\"building_id\"\n",
        "                                                              ]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlcmZlArjTGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred = df_test.loc[41697600/2:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRBG1VKvfcoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(df_pred[all_features], num_iteration=model.best_iteration)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAtCzAC3fc5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = np.expm1(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMfdMMCilmzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_results = pd.DataFrame({'row_id': df_test.row_id, 'meter_reading':np.nan})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MkerXy0mcWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_results.loc[41697600/2:, 'meter_reading'] = results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGe8lg65lmwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_results.sort_values('row_id').to_csv(\"/content/drive/My Drive/Colab Notebooks/Kaggle/data/submission_1.csv\", index=False, float_format=\"%.5f\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoh9Q4uGkk-S",
        "colab_type": "code",
        "outputId": "1f76a516-8d1d-4b54-a7f8-21174ab89208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "kf = KFold(n_splits=4, random_state=501)\n",
        "models = []\n",
        "for train_index,valid_index in kf.split(df_train):\n",
        "    X_train = df_train.reset_index(drop=True)\n",
        "    y_train = X_train[\"log_meter_reading\"]\n",
        "    y_pred_train = np.zeros(X_train.shape[0])\n",
        "    \n",
        "    X_train, X_valid = X_train.loc[train_index, all_features], X_train.loc[valid_index, all_features]\n",
        "    y_train, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "    \n",
        "    dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "    dvalid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "    print('training')\n",
        "    model = lgb.train(params, train_set=dtrain, \n",
        "                      num_boost_round=1000, \n",
        "                      valid_sets=[dtrain,dvalid], \n",
        "                      verbose_eval=25,\n",
        "                      early_stopping_rounds=50)\n",
        "    print('finished')\n",
        "    models.append(model)\n",
        "    del dtrain, dvalid"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[25]\ttraining's rmse: 1.67444\tvalid_1's rmse: 1.37563\n",
            "[50]\ttraining's rmse: 1.52847\tvalid_1's rmse: 1.27698\n",
            "[75]\ttraining's rmse: 1.41882\tvalid_1's rmse: 1.21098\n",
            "[100]\ttraining's rmse: 1.32697\tvalid_1's rmse: 1.16722\n",
            "[125]\ttraining's rmse: 1.25687\tvalid_1's rmse: 1.13716\n",
            "[150]\ttraining's rmse: 1.20141\tvalid_1's rmse: 1.1191\n",
            "[175]\ttraining's rmse: 1.15662\tvalid_1's rmse: 1.10333\n",
            "[200]\ttraining's rmse: 1.11939\tvalid_1's rmse: 1.09235\n",
            "[225]\ttraining's rmse: 1.08046\tvalid_1's rmse: 1.08576\n",
            "[250]\ttraining's rmse: 1.04829\tvalid_1's rmse: 1.07913\n",
            "[275]\ttraining's rmse: 1.02153\tvalid_1's rmse: 1.07224\n",
            "[300]\ttraining's rmse: 0.995892\tvalid_1's rmse: 1.06624\n",
            "[325]\ttraining's rmse: 0.968458\tvalid_1's rmse: 1.06272\n",
            "[350]\ttraining's rmse: 0.947679\tvalid_1's rmse: 1.06029\n",
            "[375]\ttraining's rmse: 0.936324\tvalid_1's rmse: 1.05869\n",
            "[400]\ttraining's rmse: 0.926915\tvalid_1's rmse: 1.0575\n",
            "[425]\ttraining's rmse: 0.918267\tvalid_1's rmse: 1.05605\n",
            "[450]\ttraining's rmse: 0.910966\tvalid_1's rmse: 1.05496\n",
            "[475]\ttraining's rmse: 0.90289\tvalid_1's rmse: 1.05343\n",
            "[500]\ttraining's rmse: 0.89558\tvalid_1's rmse: 1.05158\n",
            "[525]\ttraining's rmse: 0.889885\tvalid_1's rmse: 1.05167\n",
            "[550]\ttraining's rmse: 0.879728\tvalid_1's rmse: 1.05107\n",
            "[575]\ttraining's rmse: 0.86794\tvalid_1's rmse: 1.05102\n",
            "[600]\ttraining's rmse: 0.858332\tvalid_1's rmse: 1.05086\n",
            "[625]\ttraining's rmse: 0.851987\tvalid_1's rmse: 1.05075\n",
            "[650]\ttraining's rmse: 0.844933\tvalid_1's rmse: 1.0503\n",
            "[675]\ttraining's rmse: 0.83538\tvalid_1's rmse: 1.05051\n",
            "[700]\ttraining's rmse: 0.828919\tvalid_1's rmse: 1.05047\n",
            "Early stopping, best iteration is:\n",
            "[651]\ttraining's rmse: 0.844779\tvalid_1's rmse: 1.05029\n",
            "finished\n",
            "training\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[25]\ttraining's rmse: 1.62617\tvalid_1's rmse: 1.67568\n",
            "[50]\ttraining's rmse: 1.50085\tvalid_1's rmse: 1.57716\n",
            "[75]\ttraining's rmse: 1.40704\tvalid_1's rmse: 1.50429\n",
            "[100]\ttraining's rmse: 1.32982\tvalid_1's rmse: 1.43443\n",
            "[125]\ttraining's rmse: 1.26775\tvalid_1's rmse: 1.38229\n",
            "[150]\ttraining's rmse: 1.2128\tvalid_1's rmse: 1.34266\n",
            "[175]\ttraining's rmse: 1.16647\tvalid_1's rmse: 1.30516\n",
            "[200]\ttraining's rmse: 1.12422\tvalid_1's rmse: 1.27994\n",
            "[225]\ttraining's rmse: 1.08122\tvalid_1's rmse: 1.26214\n",
            "[250]\ttraining's rmse: 1.04562\tvalid_1's rmse: 1.2477\n",
            "[275]\ttraining's rmse: 1.01677\tvalid_1's rmse: 1.2395\n",
            "[300]\ttraining's rmse: 0.986165\tvalid_1's rmse: 1.23094\n",
            "[325]\ttraining's rmse: 0.967538\tvalid_1's rmse: 1.22568\n",
            "[350]\ttraining's rmse: 0.94899\tvalid_1's rmse: 1.22085\n",
            "[375]\ttraining's rmse: 0.935069\tvalid_1's rmse: 1.21941\n",
            "[400]\ttraining's rmse: 0.921812\tvalid_1's rmse: 1.21627\n",
            "[425]\ttraining's rmse: 0.910274\tvalid_1's rmse: 1.21407\n",
            "[450]\ttraining's rmse: 0.901565\tvalid_1's rmse: 1.21247\n",
            "[475]\ttraining's rmse: 0.893335\tvalid_1's rmse: 1.21202\n",
            "[500]\ttraining's rmse: 0.885745\tvalid_1's rmse: 1.21075\n",
            "[525]\ttraining's rmse: 0.877571\tvalid_1's rmse: 1.21111\n",
            "[550]\ttraining's rmse: 0.866956\tvalid_1's rmse: 1.20925\n",
            "[575]\ttraining's rmse: 0.853872\tvalid_1's rmse: 1.20902\n",
            "[600]\ttraining's rmse: 0.847008\tvalid_1's rmse: 1.20719\n",
            "[625]\ttraining's rmse: 0.83745\tvalid_1's rmse: 1.20554\n",
            "[650]\ttraining's rmse: 0.83069\tvalid_1's rmse: 1.20372\n",
            "[675]\ttraining's rmse: 0.822695\tvalid_1's rmse: 1.20312\n",
            "[700]\ttraining's rmse: 0.817037\tvalid_1's rmse: 1.20078\n",
            "[725]\ttraining's rmse: 0.809711\tvalid_1's rmse: 1.19971\n",
            "[750]\ttraining's rmse: 0.804995\tvalid_1's rmse: 1.19951\n",
            "[775]\ttraining's rmse: 0.796485\tvalid_1's rmse: 1.19836\n",
            "[800]\ttraining's rmse: 0.791821\tvalid_1's rmse: 1.19703\n",
            "[825]\ttraining's rmse: 0.78639\tvalid_1's rmse: 1.19656\n",
            "[850]\ttraining's rmse: 0.7793\tvalid_1's rmse: 1.19533\n",
            "[875]\ttraining's rmse: 0.774801\tvalid_1's rmse: 1.19461\n",
            "[900]\ttraining's rmse: 0.771461\tvalid_1's rmse: 1.19362\n",
            "[925]\ttraining's rmse: 0.766047\tvalid_1's rmse: 1.19317\n",
            "[950]\ttraining's rmse: 0.760396\tvalid_1's rmse: 1.19221\n",
            "[975]\ttraining's rmse: 0.755557\tvalid_1's rmse: 1.19237\n",
            "[1000]\ttraining's rmse: 0.751942\tvalid_1's rmse: 1.19092\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's rmse: 0.751942\tvalid_1's rmse: 1.19092\n",
            "finished\n",
            "training\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[25]\ttraining's rmse: 1.59445\tvalid_1's rmse: 1.64352\n",
            "[50]\ttraining's rmse: 1.42904\tvalid_1's rmse: 1.60406\n",
            "[75]\ttraining's rmse: 1.30385\tvalid_1's rmse: 1.57607\n",
            "[100]\ttraining's rmse: 1.19715\tvalid_1's rmse: 1.55395\n",
            "[125]\ttraining's rmse: 1.11255\tvalid_1's rmse: 1.54796\n",
            "[150]\ttraining's rmse: 1.0467\tvalid_1's rmse: 1.54969\n",
            "[175]\ttraining's rmse: 0.993143\tvalid_1's rmse: 1.54704\n",
            "[200]\ttraining's rmse: 0.942705\tvalid_1's rmse: 1.54606\n",
            "[225]\ttraining's rmse: 0.903845\tvalid_1's rmse: 1.54818\n",
            "Early stopping, best iteration is:\n",
            "[179]\ttraining's rmse: 0.984398\tvalid_1's rmse: 1.544\n",
            "finished\n",
            "training\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[25]\ttraining's rmse: 1.4249\tvalid_1's rmse: 2.12594\n",
            "[50]\ttraining's rmse: 1.26807\tvalid_1's rmse: 2.07615\n",
            "[75]\ttraining's rmse: 1.14819\tvalid_1's rmse: 2.04311\n",
            "[100]\ttraining's rmse: 1.05262\tvalid_1's rmse: 2.02073\n",
            "[125]\ttraining's rmse: 0.978873\tvalid_1's rmse: 2.00903\n",
            "[150]\ttraining's rmse: 0.922074\tvalid_1's rmse: 2.00031\n",
            "[175]\ttraining's rmse: 0.876854\tvalid_1's rmse: 1.99348\n",
            "[200]\ttraining's rmse: 0.839423\tvalid_1's rmse: 1.98856\n",
            "[225]\ttraining's rmse: 0.808533\tvalid_1's rmse: 1.98803\n",
            "[250]\ttraining's rmse: 0.783775\tvalid_1's rmse: 1.98533\n",
            "[275]\ttraining's rmse: 0.760467\tvalid_1's rmse: 1.98035\n",
            "[300]\ttraining's rmse: 0.740295\tvalid_1's rmse: 1.97584\n",
            "[325]\ttraining's rmse: 0.723265\tvalid_1's rmse: 1.97339\n",
            "[350]\ttraining's rmse: 0.708936\tvalid_1's rmse: 1.97046\n",
            "[375]\ttraining's rmse: 0.695908\tvalid_1's rmse: 1.96864\n",
            "[400]\ttraining's rmse: 0.683307\tvalid_1's rmse: 1.96668\n",
            "[425]\ttraining's rmse: 0.672997\tvalid_1's rmse: 1.96594\n",
            "[450]\ttraining's rmse: 0.664991\tvalid_1's rmse: 1.96481\n",
            "[475]\ttraining's rmse: 0.657507\tvalid_1's rmse: 1.96439\n",
            "[500]\ttraining's rmse: 0.651399\tvalid_1's rmse: 1.96387\n",
            "[525]\ttraining's rmse: 0.646332\tvalid_1's rmse: 1.96389\n",
            "[550]\ttraining's rmse: 0.640926\tvalid_1's rmse: 1.96381\n",
            "[575]\ttraining's rmse: 0.635497\tvalid_1's rmse: 1.96356\n",
            "[600]\ttraining's rmse: 0.631191\tvalid_1's rmse: 1.96276\n",
            "[625]\ttraining's rmse: 0.625828\tvalid_1's rmse: 1.96144\n",
            "[650]\ttraining's rmse: 0.622146\tvalid_1's rmse: 1.96167\n",
            "[675]\ttraining's rmse: 0.618255\tvalid_1's rmse: 1.96164\n",
            "[700]\ttraining's rmse: 0.614094\tvalid_1's rmse: 1.96097\n",
            "[725]\ttraining's rmse: 0.610691\tvalid_1's rmse: 1.9607\n",
            "[750]\ttraining's rmse: 0.606984\tvalid_1's rmse: 1.96042\n",
            "[775]\ttraining's rmse: 0.603616\tvalid_1's rmse: 1.96011\n",
            "[800]\ttraining's rmse: 0.600729\tvalid_1's rmse: 1.96026\n",
            "Early stopping, best iteration is:\n",
            "[765]\ttraining's rmse: 0.604671\tvalid_1's rmse: 1.95998\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40r0Ux-vGM_E",
        "colab_type": "code",
        "outputId": "a5466d6c-a73f-46cc-866f-8d53b4bb74b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del build, df_train, train\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWWefienkk-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = [0] * len(df_test)\n",
        "for model in models:\n",
        "    results = results + np.expm1(model.predict(df_test[all_features], num_iteration=model.best_iteration)) / len(models)\n",
        "    print('Terminou modelo {}'.format(model))\n",
        "    del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbPJZFb0kk-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_df = pd.DataFrame({\"row_id\": X.index, \"meter_reading\": np.clip(results, 0, a_max=None)})\n",
        "len(results_df)\n",
        "results_df.to_csv(\"/content/drive/My Drive/Colab Notebooks/Kaggle/data/submission_1.csv\", index=False, float_format=\"%.5f\")\n",
        "del results "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFt6A_d8kk-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-8ozPv0yy1j",
        "colab_type": "text"
      },
      "source": [
        "**Neural Network over the entire dataset**\n",
        "\n",
        "In this section the neural network is used in an attempt to predict the meters' values over the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cneDfyWdkk-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The remaining code is used for the actual training of the network. It is based on a standard procedure for training and predictions using randomized\n",
        "parts of the original dataset\n",
        "'''\n",
        "oof = np.zeros(len(train))\n",
        "batch_size = 1024\n",
        "epochs = 100\n",
        "models = []\n",
        "\n",
        "folds = 4\n",
        "seed = 666\n",
        "\n",
        "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "\n",
        "for fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['building_id'])):\n",
        "    print('Fold:', fold_n)\n",
        "    X_train, X_valid = df_train[all_features].iloc[train_index], df_train[all_features].iloc[valid_index]\n",
        "    y_train, y_valid = df_train['log_meter_reading'].iloc[train_index], df_train['log_meter_reading'].iloc[valid_index]\n",
        "    X_t = get_keras_data(X_train, numericals, categoricals)\n",
        "    X_v = get_keras_data(X_valid, numericals, categoricals)\n",
        "    \n",
        "    keras_model = model(dense_dim_1=128, dense_dim_2=64, dense_dim_3=32, dense_dim_4=32, dense_dim_5=16, \n",
        "                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.01)\n",
        "    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n",
        "    models.append(mod)\n",
        "    print('*'* 50)\n",
        "\n",
        "from tqdm import tqdm\n",
        "i=0\n",
        "res = np.zeros((test.shape[0]),dtype=np.float32)\n",
        "step_size = 50000\n",
        "for j in tqdm(range(int(np.ceil(test.shape[0]/step_size)))):\n",
        "    for_prediction = get_keras_data(test.iloc[i:i+step_size], numericals, categoricals)\n",
        "    res[i:min(i+step_size,test.shape[0])] = \\\n",
        "       np.expm1(sum([model.predict(for_prediction, batch_size=1024)[:,0] for model in models])/folds)\n",
        "    i+=step_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfXi1kRskk-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsDXbF_4kk-k",
        "colab_type": "text"
      },
      "source": [
        "**Xgbook per site**\n",
        "\n",
        "In this section it is attempted to predict the meter values separatedly for each site ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yonUoJ1kk-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = 2\n",
        "models = {}\n",
        "cv_scores = {\"site_id\": [], \"cv_score\": []}\n",
        "\n",
        "for site_id in tqdm(range(16), desc=\"site_id\"):\n",
        "    print(cv, \"fold CV for site_id:\", site_id)\n",
        "    kf = KFold(n_splits=cv, random_state=seed)\n",
        "    models[site_id] = []\n",
        "\n",
        "    X_train_site = df_train[df_train[\"site_id\"]==site_id].reset_index(drop=True)\n",
        "    y_train_site = X_train_site[\"log_meter_reading\"]\n",
        "    y_pred_train_site = np.zeros(X_train_site.shape[0])\n",
        "    \n",
        "    score = 0\n",
        "\n",
        "    for fold, (train_index, valid_index) in enumerate(kf.split(X_train_site, y_train_site)):\n",
        "        X_train, X_valid = X_train_site.loc[train_index, all_features], X_train_site.loc[valid_index, all_features]\n",
        "        y_train, y_valid = y_train_site.iloc[train_index], y_train_site.iloc[valid_index]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        dvalid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "\n",
        "        watchlist = [dtrain, dvalid]\n",
        "\n",
        "        params = {\"objective\": \"regression\",\n",
        "                  \"num_leaves\": 41,\n",
        "                  \"learning_rate\": 0.049,\n",
        "                  \"bagging_freq\": 5,\n",
        "                  \"bagging_fraction\": 0.51,\n",
        "                  \"feature_fraction\": 0.81,\n",
        "                  \"metric\": \"rmse\"\n",
        "                  }\n",
        "\n",
        "        model_lgb = lgb.train(params, train_set=dtrain, \n",
        "                              num_boost_round=999, \n",
        "                              valid_sets=watchlist, \n",
        "                              verbose_eval=101, \n",
        "                              early_stopping_rounds=50)\n",
        "        models[site_id].append(model_lgb)\n",
        "\n",
        "        y_pred_valid = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n",
        "        y_pred_train_site[valid_index] = y_pred_valid\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
        "        print(\"Site Id:\", site_id, \", Fold:\", fold+1, \", RMSE:\", rmse)\n",
        "        score += rmse / cv\n",
        "        \n",
        "        gc.collect()\n",
        "        \n",
        "    cv_scores[\"site_id\"].append(site_id)\n",
        "    cv_scores[\"cv_score\"].append(score)\n",
        "        \n",
        "    print(\"\\nSite Id:\", site_id, \", CV RMSE:\", np.sqrt(mean_squared_error(y_train_site, y_pred_train_site)), \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJHyBP4Nkk-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_sites = []\n",
        "\n",
        "for site_id in tqdm(range(16), desc=\"site_id\"):\n",
        "    print(\"Preparing test data for site_id\", site_id)\n",
        "\n",
        "    X_test_site = df_test[df_test.site_id==site_id][all_features]\n",
        "    \n",
        "    row_ids_site = X_test_site.row_id\n",
        "\n",
        "    X_test_site = X_test_site[all_features]\n",
        "    y_pred_test_site = np.zeros(X_test_site.shape[0])\n",
        "\n",
        "    print(\"Scoring for site_id\", site_id)    \n",
        "    for fold in range(cv):\n",
        "        model_lgb = models[site_id][fold]\n",
        "        y_pred_test_site += model_lgb.predict(X_test_site, num_iteration=model_lgb.best_iteration) / cv\n",
        "        gc.collect()\n",
        "        \n",
        "    df_test_site = pd.DataFrame({\"row_id\": row_ids_site, \"meter_reading\": y_pred_test_site})\n",
        "    df_test_sites.append(df_test_site)\n",
        "    \n",
        "    print(\"Scoring for site_id\", site_id, \"completed\\n\")\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-dhZ4e8kk-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submit = pd.concat(df_test_sites)\n",
        "submit.meter_reading = np.clip(np.expm1(submit.meter_reading), 0, a_max=None)\n",
        "submit.to_csv(\"submission_diogo_10_modelos_siteid.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}